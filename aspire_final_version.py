# -*- coding: utf-8 -*-
"""Aspire_Final_Version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18cp1nzEDEY7_AuGeM8Oz8muDmZllvp1k
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("tea340yashjoshi/skill-and-career-recommendation-dataset")

print("Path to dataset files:", path)



import os
import pandas as pd

# List files in the dataset directory
dataset_path = '/kaggle/input/skill-and-career-recommendation-dataset'
print("Files in dataset directory:", os.listdir(dataset_path))

# Presuming that the primary data file is an Excel file. # You may need to change the filename below if there are several files.
# Based on the output, let's assume that the file is called 'Dataset Project 404.xlsx'.
file_name = 'Dataset Project 404.xlsx'
file_path = os.path.join(dataset_path, file_name)

try:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")

    # Open a DataFrame and read the Excel file.
    df = pd.read_excel(file_path)
    print("\nDataFrame created successfully. First 5 rows:")
    display(df.head())
except FileNotFoundError as e:
    print(f"Error: {e}")
    print("Please check the dataset directory and specify the correct file name if needed.")
except Exception as e:
    print(f"An error occurred while reading the Excel file: {e}")

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import numpy as np


# Drop the 'Course' column
df = df.drop(['Course'], axis=1)
df=df.drop(['Sr.No.'],axis=1)
df=df.drop(['s/p'],axis=1)
df=df.drop(['Student'],axis=1)

# Identify categorical and numerical columns (excluding the target)
categorical_features = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8']
numerical_features = ['Linguistic', 'Musical', 'Bodily', 'Logical - Mathematical', 'Spatial-Visualization', 'Interpersonal', 'Intrapersonal', 'Naturalist']

# Define the order for the ordinal encoder
categories_order = ['POOR', 'AVG', 'BEST']
categorical_categories = [categories_order] * len(categorical_features)

# Separate the target variable before preprocessing
Y = df['Job profession']
X = df.drop('Job profession', axis=1)

# Handle missing values in categorical features before defining the preprocessor
X[categorical_features] = X[categorical_features].fillna('Unknown')

# Create the column transformer for preprocessing features
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OrdinalEncoder(categories=categorical_categories, handle_unknown='use_encoded_value', unknown_value=-1), categorical_features),
        ('num', StandardScaler(), numerical_features)
    ],
    remainder='drop' # Drop columns not specified in transformers (like Sr.No., Student, s/p)
)

# Encode the target variable
label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)


# Create the pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y_encoded, test_size=0.2, random_state=42)

# Train the pipeline
pipeline.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred_pipeline = pipeline.predict(X_test)

# Evaluate the pipeline
accuracy_pipeline = accuracy_score(Y_test, Y_pred_pipeline)
report_pipeline = classification_report(Y_test, Y_pred_pipeline, target_names=label_encoder.classes_)

print(f"Pipeline Accuracy: {accuracy_pipeline}")

print("Pipeline Classification Report:\n", report_pipeline)

from sklearn.model_selection import cross_val_score

# Perform cross-validation on the pipeline
cv_scores_pipeline = cross_val_score(pipeline, X_train, Y_train, cv=10) # Using 5-fold cross-validation

print("Cross-validation scores for the pipeline:", cv_scores_pipeline)
print("Mean cross-validation accuracy for the pipeline:", cv_scores_pipeline.mean())

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for the Random Forest Classifier
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit Grid Search to the training data
grid_search.fit(X_train, Y_train)

# Print the best parameters and best score
print("Best parameters found:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)

# Evaluate the best model on the test set
best_pipeline = grid_search.best_estimator_
Y_pred_best_pipeline = best_pipeline.predict(X_test)
accuracy_best_pipeline = accuracy_score(Y_test, Y_pred_best_pipeline)
report_best_pipeline = classification_report(Y_test, Y_pred_best_pipeline, target_names=label_encoder.classes_)

print(f"Test set accuracy with best parameters: {accuracy_best_pipeline}")
print("Test set Classification Report with best parameters:\n", report_best_pipeline)

from google.colab import drive
drive.mount('/content/drive')
# Then copy the file to your drive
!cp best_pipeline_model.pkl /content/drive/My\ Drive/

